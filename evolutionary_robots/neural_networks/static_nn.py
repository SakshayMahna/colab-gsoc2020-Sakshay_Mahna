"""Docstring for the static_nn.py module

This module implements the Static Neural Networks
The fundamental idea behind neural networks is the concept of layers
A collection of layers make up a Neural Network
So, we implement a layer class and then a neural network class that makes use of it!

"""

import numpy as np
import pickle
from graphviz import Digraph
from layers import StaticLayer

# Library used to generate warnings
import warnings

# Note: The weight matrix works as follows:
# Each of the columns tune for the weights of a single input node
# Each of the rows tune for the weights of a single output node

# Static Neural Network Class
class StaticNeuralNetwork:
	"""
	The Static Neural Network Class
	The class has adjustable number of layers, number of neurons in a layer,
	activation function and the parameters of those activation functions.
	In essence a Neural Network is a collection of Layers
	
	...
	
	Attributes
	----------
	layer_dimensions: array_like
		The layer_dimensions specifies the parameters of the layers of the Neural Network.
		The layout followed is:
		[[number_of_nodes_in_first_layer(input layer), activation_class], [number_of_nodes_in_second_layer, activation_class], ..., [number_of_output_nodes]]
		
	Methods
	-------
	forward_propagate(input_vector)
		Generates the output of the Neural Network given the input_vector
		
	save_parameters_to_file(file_name)
		Saves the parameters of the Neural Network to an external file named as file_name
		
	load_parameters_from_file(file_name)
		Loads the parameters of the Neural Network from an external file named as file_name
		
	return_parameters_as_vectors()
		Returns the parameters of the vector organized in the form of a vector
		
	load_parameters_from_vector(weight_vector)
		Load the parameters of the Neural Network from a user input in the form of an array/vector
		
	generate_visual(filename, view=False)
		Generate the visual representation of the Neural Network
	"""

	# The layer_dimensions is an array with the following layout
	# [[number_of_nodes_in_first_layer(input layer), activation_function], [number_of_nodes_in_second_layer, activation_function], ..., [number_of_output_nodes]]
	def __init__(self, layer_dimensions):
		# Make some class variable declarations
		self.input_dim = (layer_dimensions[0][0], )
	
		# Initialize a layer_vector, that is a list of Layer objects
		self.layer_vector = []
		
		# Append the Layer classes
		for i in range(len(layer_dimensions) - 1):
			self.layer_vector.append(StaticLayer(layer_dimensions[i][0], layer_dimensions[i+1][0], layer_dimensions[i][1], "Layer_"+str(i)))
			
		# Number of layers
		self.number_of_layers = len(self.layer_vector)
		# Construct a visual component
		self.visual = Digraph(comment="Static Neural Network", graph_attr={'rankdir': "LR", 'splines': "line"}, node_attr={'fixedsize': "true", 'label': ""})
		# self.generate_visual(len(self.layer_vector))
		
	# Function to get output from input_vector
	def forward_propagate(self, input_vector):
		"""
		Generate the output of the Neural Network when input_vector
		is passed to the Neural Network
		
		Parameters
		----------
		input_vector: array_like
			The input_vector to be passed to the Neural Network
			
		Returns
		-------
		intermediate_output: array_like
			The output vector that is generated by the Neural Network
			
		Raises
		------
		ValueException
			The input_vector should be of the dimensions of the input of the network
			
		Notes
		-----
		The Neural Network is a collection of layers. To generate the output of the
		Neural Network the input_vector is fed into the first layer, whose output is
		then fed into the next layer. Successively doing this for each layer, generates
		the output of the network.
		"""
		# Convert the input_vector to numpy array
		intermediate_output = np.array(input_vector)
		
		# Check for dimensions
		if(intermediate_output.shape != self.input_dim):
			raise ValueError("The dimensions of the input vector do not match to the specified ones!")
		
		# Forward propagate for each layer
		for layer in self.layer_vector:
			intermediate_output = layer.forward_propagate(intermediate_output)
			
		return intermediate_output
		
	# Function to save the layer parameters
	def save_parameters_to_file(self, file_name):
		"""
		Save the parameters of the Neural Network
		
		Using pickle, the list of layers is stored
		"""
		# Use pickle to save the layer_vector
		with open(file_name, 'wb') as f:
			pickle.dump(self.layer_vector, f)
			
	# Function to load the layer parameters
	def load_weights_from_file(self, file_name):
		""" Load the parameters of the Neural Network """
		# Use pickle to load the layer_vector
		with open(file_name, 'rb') as f:
			self.layer_vector = pickle.load(f)
			
	# Function to return the parameters in the form of a vector
	def return_parameters_as_vector(self):
		"""
		Return the parameters of the Static Neural Network in the form of
		a an array / vector.
		
		Parameters
		----------
		None
		
		Returns
		-------
		output: array_like
			The vector representation of the parameters of the Neural Network
			
		Raises
		------
		None
		
		Notes
		-----
		The numpy flatten function works in row major order.
		The parameter vector follows the layout as
		[w_11, w_21, w_12, w_22, w_13, w_23, b_1, b_2, b_3, a_1g, a_1b, a_2g, a_2b, a_3g, a_3b, w_11, w_21, w_31, b_1, ...]
		Here, w_ij implies the weight between ith input node and jth output node. b_i is the bias for the ith output node.
		a_ib is the bias activation parameter of ith output node and a_ig is the gain activation parameter of ith output node. 
		"""
		# Initialize the output vector
		# Determine an individual layer's weight matrix in row major form and then it's bias
		# Then concatenate it with the previous output vector
		output = np.array([])
	
		for layer in self.layer_vector:
			# The vector we get from flattening the weight matrix
			# flatten() works in row major order
			weight_vector = layer.get_weight_matrix().flatten()
			
			# The vector we get from flattening the bias vector
			bias_vector = layer.get_bias_vector().flatten()
			
			# The vector of activation parameters
			activation_vector = np.array(layer.get_activation_parameters())
			
			# The output vector is concatenated form of weight_vector, bias_vector and activation_vector
			output = np.concatenate([output, weight_vector, bias_vector, activation_vector])
		
		return output
	
	# Function to load the parameters from vector
	def load_parameters_from_vector(self, parameter_vector):
		"""
		Load the parameters of the Static Neural Network in the form of
		an array / vector
		
		Parameters
		----------
		parameter_vector: array_like
			The parameter vector follows the layout as
			[w_11, w_21, w_12, w_22, w_13, w_23, b_1, b_2, b_3, a_1g, a_1b, a_2g, a_2b, a_3g, a_3b, w_11, w_21, w_31, b_1, ...]
			Here, w_ij implies the weight between ith input node and jth output node. b_i is the bias for the ith output node.
			a_ib is the bias activation parameter of ith output node and a_ig is the gain activation parameter of ith output node. 
			
		Returns
		-------
		None
		
		Raises
		------
		ValueException
			The parameter array is shorter than required
			
		Warning
			The parameter array is greater than required
			
		"""
		# Convert to numpy array
		parameter_vector = np.array(parameter_vector)
	
		# Interval counter maintains the current layer index
		interval_counter = 0
		
		for layer in self.layer_vector:
			# Get the dimensions of the weight matrix and bias vector
			weight_dim = layer.get_weight_dim()
			bias_dim = layer.get_bias_dim()
			
			# Get the interval at which weight and bias seperate
			weight_interval = weight_dim[0] * weight_dim[1]
			
			# Get the interval at which the bias and next weight vector seperate
			bias_interval = bias_dim[0]
			
			# Seperate the weights and bias and then reshape them
			# Numpy raises a None Type Exception, as it cannot reshape a None object
			# If such an excpetion occurs, raise a value error as our parameter_vector
			# is shorter than required
			try:
				layer.set_weight_matrix(parameter_vector[interval_counter:interval_counter + weight_interval].reshape(weight_dim))
				interval_counter = interval_counter + weight_interval
				layer.set_bias_vector(parameter_vector[interval_counter:interval_counter + bias_interval].reshape(bias_dim[0],))
				interval_counter = interval_counter + bias_interval
				layer.set_activation_parameters(parameter_vector[interval_counter], parameter_vector[interval_counter + 1])
				interval_counter = interval_counter + 2
			except:
				raise ValueError("The parameter_vector consists of elements less than required")
			
		# The interval counter should contain the number of elements in parameter_vector
		# Otherwise the user has specified parameters more than required
		# Just a warning is enough
		if(len(parameter_vector) > interval_counter):
			warnings.warn("The parameter vector consists of elements greater than required")
			
	
	# Function to generate the visual representation
	def generate_visual(self, filename, view=False):
		"""
		Generate the visual representation of the Neural Network and
		store it as a pdf file in the representations directory
		
		Parameters
		----------
		filename
			Specifies the name of the pdf file which is to be saved
			
		view=False
			Whether to view the generated file(True) or not(False)
			
		Returns
		-------
		None
		
		Raises
		------
		None
		
		Note
		----
		Graphviz library is used to generate the representation
		"""
		# We need many subgraphs
		for layer in range(self.number_of_layers):
			subgraph = Digraph(name="cluster_" + str(layer), graph_attr={'color': "white", 'label': "Layer " + str(layer)}, node_attr={'style': "solid", 'color': "black", 'shape': "circle"})
			
			# Get the weight dimensions for generating the nodes
			weight_dim = self.layer_vector[layer].get_weight_dim()
			
			for node_number in range(weight_dim[1]):
				subgraph.node("layer_" + str(layer) + str(node_number+1))
				
			# Declare subgraphs
			self.visual.subgraph(subgraph)
			
			
		# The final layer needs to be done manually
		subgraph = Digraph(name="cluster_" + str(self.number_of_layers), graph_attr={'color': "white", 'label': "Layer " + str(self.number_of_layers)}, node_attr={'style': "solid", 'color': "black", 'shape': "circle"})
		
		# Get the weight dimensions
		weight_dim = self.layer_vector[self.number_of_layers - 1].get_weight_dim()
		
		for node_number in range(weight_dim[0]):
			subgraph.node("layer_" + str(self.number_of_layers) + str(node_number+1))
			
		# Declare the subgraph
		self.visual.subgraph(subgraph)
		
		
		for layer in range(self.number_of_layers):
			# Get the weight dimensions for generating the nodes
			weight_dim = self.layer_vector[layer].get_weight_dim()
		
			# Put the edges in the graph
			for input_node in range(weight_dim[1]):
				for output_node in range(weight_dim[0]):
					self.visual.edge("layer_" + str(layer) + str(input_node+1), 'layer_' + str(layer + 1) + str(output_node+1))
		
		# Render the graph		
		self.visual.render('representations/' + filename + '.gv', view=view)	
		
		
	
		
		
		
	
		
		
