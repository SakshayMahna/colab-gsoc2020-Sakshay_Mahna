"""Docstring for dynamic_nn.py module

This module contains the Dynamic Neural Network class
Two main constituents of the Dynamic Layer are Time Delay and Time Recurrence
Time Delay provides the current output as well as the previous input to the network
Time Recurrence, provides the output of a layer to the input of a previous layer

The dynamic neural network implemented here only work for left-to-right case
For a network with right_to_left connections, the user is recommended to construct
a custom neural network without any abstractions!
"""

import numpy as np
import pickle
from graphviz import Digraph
from layers import DynamicLayer

# Library used to generate warnings
import warnings
		
# The Dynamic Neural Network class
class DynamicNeuralNetwork:
	"""
	The Dynamic Neural Network class
	The class has adjustable number of layers, neurons in each layer,
	the activation function and delay of each layer along with the
	recurrent connections between layers
	
	...
	
	Attributes
	----------
	layer_dimensions: array_like
		The layer dimensions specify the parameters of the layers of the Neural Network
		The layout followed is:
		[[nodes_in_layer_one, delay_dim, [list_of_connections], activation_function], [nodes_in_layer_two, delay_dim, [list_of_connections], activation_function], ...[nodes_in_output]]
		The list_of_connections is provided in a left to right fashion
		
	
	Methods
	-------
	forward_propagate(input_vector)
		Generates the output of the Neural Network given the input_vector
		
	save_parameters_to_file(file_name)
		Saves the parameters of the Neural Network to an external file named as file_name
		
	load_parameters_from_file(file_name)
		Loads the parameters of the Neural Network from an external file named as file_name
		
	return_parameters_as_vectors()
		Returns the parameters of the vector organized in the form of a vector
		
	load_parameters_from_vector(weight_vector)
		Load the parameters of the Neural Network from a user input in the form of an array/vector
		
	generate_visual(filename, view=False)
		Generate the visual representation of the Neural Network
		
	Additional Methods
	------------------
	update_inputs()
		Internal method to update the inputs of the recurrent system
		
	process_connections()
		Internal method to classify the input connections and output connections
	"""
	def __init__(self, layer_dimensions):
		# Make some class variable declarations
		self.input_dim = (layer_dimensions[0][0], )
		
		# Number of layers
		self.number_of_layers = len(layer_dimensions) - 1
		
		# Initialize a layer vector
		self.layer_vector = []
		
		# Parse the list of connections
		self.process_connections(layer_dimensions)
		
		# Generate a list of layers
		for i in range(self.number_of_layers):
			# For the generation of recurrent_dim list
			recurrent_dim = []
			for connection in self.input_connections[i]:
				# Append the dimensions of the output of the layer that is mentioned in the input_connections
				# The user specifies the layer whose output is required
				recurrent_dim.append(layer_dimensions[connection + 1][0])
			self.layer_vector.append(DynamicLayer(layer_dimensions[i][0], layer_dimensions[i+1][0], layer_dimensions[i][1], recurrent_dim, layer_dimensions[i][3], "Layer_"+str(i)))
			
		# Construct a visual component
		self.visual = Digraph(comment="Dynamic Neural Network", graph_attr={'rankdir': "LR", 'splines': "line"}, node_attr={'fixedsize': "true", 'label': ""})
			
	# Function to forward propagate the output
	def forward_propagate(self, input_vector):
		"""
		Generate the output of the Neural Network when input_vector
		is passed to the Neural Network
		
		Parameters
		----------
		input_vector: array_like
			The input_vector to be passed to the Neural Network
			
		Returns
		-------
		intermediate_output: array_like
			The output vector that is generated by the Neural Network
			
		Raises
		------
		ValueException
			The input_vector should be of the dimensions of the input of the network
			
		Notes
		-----
		In Dynamic Neural Networks, all the layer outputs are stored. This makes it easy
		to update the input vectors of the recurrent system.
		"""
		# Convert to numpy array
		input_vector = np.array(input_vector)
		
		# CHeck shape
		if(input_vector.shape != self.input_dim):
			raise ValueError("The dimensions of the input vector do not match to the specified ones!")
		
		# A list of intermediate outputs will enable us to update the input_vector for each layer after calculation of the whole output
		self.intermediate_output = [np.array([])] * self.number_of_layers
		
		# The first output is calculated without the loop to start with the intermediate output list
		self.intermediate_output[0] = self.layer_vector[0].forward_propagate(input_vector)
		
		# The loop to generate the intermediate output list
		for layer in range(1, self.number_of_layers):
			self.intermediate_output[layer] = self.layer_vector[layer].forward_propagate(self.intermediate_output[layer-1])
			
		# Update the Neural Network inputs
		self.update_inputs()	
		
		# Return the output
		return self.intermediate_output[self.number_of_layers - 1]
		
	# Function to update the inputs
	def update_inputs(self):
		""" Internal function to update the recurrent weights. Works in left-to-right fashion """
		# Loop through the input_connections
		for i in range(len(self.input_connections)):
			# Variable to help update the input from left to right
			update_index = 0
			for connections in self.input_connections[i]:
				# Keep updating the recurrent_input in a left to right fashion
				self.layer_vector[i].set_recurrent_input(self.intermediate_output[connections].flatten(), update_index)
				update_index = update_index + 1
	
	# Function to process the connections that are input and output
	def process_connections(self, layer_dimensions):
		""" Internal function to seperate the output connections and the input connections """
		# Initialize the input and output connection list
		self.input_connections = []
		self.output_connections = []
		
		# If the current layer has a connection with a layer to it's right, then it is an input for that layer
		# If the current layer has a connection with a layer to it's left, then it is an output for that layer
		for i in range(self.number_of_layers):
			in_connections = []
			out_connections = []
			for j in layer_dimensions[i][2]:
				if j >= i:
					in_connections.append(j)
				else:
					out_connections.append(j)
					
			self.input_connections.append(in_connections)
			self.output_connections.append(out_connections)
	
	# Function to save the layer parameters
	def save_parameters_to_file(self, file_name):
		"""
		Save the parameters of the Neural Network
		
		Using pickle, the list of layers is stored
		"""
		# Use pickle to save the layer_vector
		# This even saves all the previous input we were working on!
		with open(filename, 'wb') as f:
			pickle.dump(self.layer_vector, f)
			
	# Function to load the layer parameters
	def load_parameters_from_file(self, file_name):
		""" Load the parameters of the Neural Network """
		# Use pickle to load the layer_vector
		with open(file_name, 'rb') as f:
			self.layer_vector = pickle.load(f)
			
	# Function to return all the parameters in the form of a vector
	def return_parameters_as_vector(self):
		"""
		Return the parameters of the Dynamic Neural Network in the form of
		a an array / vector.
		
		Parameters
		----------
		None
		
		Returns
		-------
		output: array_like
			The vector representation of the parameters of the Neural Network
			
		Raises
		------
		None
		
		Notes
		-----
		The numpy flatten function works in row major order.
		The layout followed is the same as that of Static Neural Network, with a few additions
		weights of delay system + weights of recurrent system + weights of static system + weights of bias + activation function parameters
		"""
		
		# Initialize the output
		output = np.array([])
		
		for layer in range(self.number_of_layers):
			# The delay system uses a weight vector
			weight_vector_delay = self.layer_vector[layer].get_delay_weight_vector().flatten()
			
			# The recurrent weights need to be flattened and collected as well
			# They are taken from input side
			weight_vector_recurrent = np.array([])
			for index in range(len(self.input_connections[layer])):
				weight_vector_recurrent = np.concatenate([weight_vector_recurrent, self.layer_vector[layer].get_recurrent_weight_matrix(index).flatten()])
				
			# Get the static weight matrix
			weight_vector_static = self.layer_vector[layer].get_weight_matrix().flatten()
			
			# Get the bias vector
			bias_vector = self.layer_vector[layer].get_bias_vector().flatten()
			
			# Get the activation parameters
			activation_vector = np.array(self.layer_vector[layer].get_activation_parameters())
			
			# concatenate everything
			output = np.concatenate([output, weight_vector_delay, weight_vector_recurrent, weight_vector_static, bias_vector, activation_vector])
			
		return output
		
	# Function to set all the parameters from a vector
	def load_parameters_from_vector(self, parameter_vector):
		"""
		Load the parameters of the Dynamic Neural Network in the form of
		an array / vector
		
		Parameters
		----------
		parameter_vector: array_like
			The layout followed is the same as that of Static Neural Network, with a few additions
		weights of delay system + weights of recurrent system + weights of static system + weights of bias + activation function parameters
			
		Returns
		-------
		None
		
		Raises
		------
		ValueException
			The parameter array is shorter than required
			
		Warning
			The parameter array is greater than required
		"""
		# Same layout, therefore we need to extract and then load!
		# Convert to numpy array
		parameter_vector = np.array(parameter_vector)
		
		# Interval counter maintains the current layer index
		interval_counter = 0
		
		# Numpy raises a None Type Exception, as it cannot reshape a None object
		# If such an excpetion occurs, raise a value error as our parameter_vector
		# is shorter than required
		try:
			# Contrary to static neural network, in this case, we extract dimensions and then extract the weight simultaneously 
			for layer in range(self.number_of_layers):
				# Get the dimensions, interval and extract for delay
				delay_dim = self.layer_vector[layer].get_delay_weight_dim()
				delay_interval = delay_dim[0] * delay_dim[1]
				self.layer_vector[layer].set_delay_weight_vector(parameter_vector[interval_counter:interval_counter+delay_interval].reshape(delay_dim))
				interval_counter = interval_counter + delay_interval
				
				# Get the dimensions, interval and extract for recurrent
				for index in range(len(self.input_connections[layer])):
					recurrent_dim = self.layer_vector[layer].get_recurrent_weight_dim(index)
					recurrent_interval = recurrent_dim[0] * recurrent_dim[1]
					self.layer_vector[layer].set_recurrent_weight_matrix(parameter_vector[interval_counter:interval_counter+recurrent_interval].reshape(recurrent_dim), index)
					interval_counter = interval_counter + recurrent_interval
					
				# Get the dimensions, interval and extract for static weight
				weight_dim = self.layer_vector[layer].get_weight_matrix_dim()
				weight_interval = weight_dim[0] * weight_dim[1]
				self.layer_vector[layer].set_weight_matrix(parameter_vector[interval_counter:interval_counter+weight_interval].reshape(weight_dim))
				interval_counter = interval_counter + weight_interval
				
				# Get the dimensions, interval and extract for bias vector
				bias_dim = self.layer_vector[layer].get_bias_vector_dim()
				bias_interval = bias_dim[0]
				self.layer_vector[layer].set_bias_vector(parameter_vector[interval_counter:interval_counter+bias_interval].reshape(bias_dim[0],))
				interval_counter = interval_counter + bias_interval
				
				# Get the interval and extract for activation vector
				activation_interval = 2
				self.layer_vector[layer].set_activation_parameters(parameter_vector[interval_counter], parameter_vector[interval_counter+1])
				interval_counter = interval_counter + activation_interval
				
		except:
			raise ValueError("The parameter_vector consists of elements less than required")
			
		# The interval counter should contain the number of elements in parameter_vector
		# Otherwise the user has specified parameters more than required
		# Just a warning is enough
		if(len(parameter_vector) > interval_counter):
			warnings.warn("The parameter vector consists of elements greater than required")
			
	
	# Function to generate the visual representation
	def generate_visual(self, filename, view=False):
		"""
		Generate the visual representation of the Neural Network and
		store it as a pdf file in the representations directory
		
		Parameters
		----------
		filename
			Specifies the name of the pdf file which is to be saved
			
		view=False
			Whether to view the generated file(True) or not(False)
			
		Returns
		-------
		None
		
		Raises
		------
		None
		
		Note
		----
		Graphviz library is used to generate the representation
		"""
		# We need many subgraphs
		for layer in range(self.number_of_layers):
			# Delay dimension
			delay = self.layer_vector[layer].get_delay_weight_dim()
			subgraph = Digraph(name="cluster_" + str(layer), graph_attr={'color': "white", 'label': "Layer " + str(layer) + "\nDelay " + str(delay[1])}, node_attr={'style': "solid", 'color': "black", 'shape': "circle"})
			
			# Get the weight dimensions for generating the nodes
			weight_dim = self.layer_vector[layer].get_weight_matrix_dim()
			
			for node_number in range(weight_dim[1]):
				subgraph.node("layer_" + str(layer) + str(node_number+1))
				
			# Declare subgraphs
			self.visual.subgraph(subgraph)
			
			
		# The final layer needs to be done manually
		subgraph = Digraph(name="cluster_" + str(self.number_of_layers), graph_attr={'color': "white", 'label': "Output"}, node_attr={'style': "solid", 'color': "black", 'shape': "circle"})
		
		# Get the weight dimensions
		weight_dim = self.layer_vector[self.number_of_layers - 1].get_weight_matrix_dim()
		
		for node_number in range(weight_dim[0]):
			subgraph.node("layer_" + str(self.number_of_layers) + str(node_number+1))
			
		# Declare the subgraph
		self.visual.subgraph(subgraph)
		
		
		for layer in range(self.number_of_layers):
			# Get the weight dimensions for generating the nodes
			weight_dim = self.layer_vector[layer].get_weight_matrix_dim()
		
			# Put the edges in the graph
			for input_node in range(weight_dim[1]):
				for output_node in range(weight_dim[0]):
					self.visual.edge("layer_" + str(layer) + str(input_node+1), 'layer_' + str(layer + 1) + str(output_node+1))
		
		# Add the recurrent edges
		for recurrence_input in range(len(self.input_connections)):
			for recurrence_output in self.input_connections[recurrence_input]:
				input_weight_dim = self.layer_vector[recurrence_input].get_weight_matrix_dim()
				output_weight_dim = self.layer_vector[recurrence_output].get_weight_matrix_dim()
				self.visual.edge("layer_" + str(recurrence_output + 1) + str(output_weight_dim[0]), "layer_" + str(recurrence_input + 1) + str(input_weight_dim[0]), style = 'dashed', color="red", constraint='false')
		
		# Render the graph		
		self.visual.render('representations/' + filename + '.gv', view=view)
			
	
