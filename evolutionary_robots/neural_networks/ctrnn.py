"""Docstring for ctrnn.py module

This module contains the implementation of Continuous Time Recurrent Neural Network
The network is more or less similar to the Static Neural Network
Making the time interval equal to the time constant gives us the Static Neural Network

CTRNNs work based on a differential equation
In terms of our computer, the differential equation is modelled in discrete manner
In this implementation the discrete manner is first order euler solution
But the user can change this implementation in any way he/she likes!
"""

import numpy as np
import pickle
from graphviz import Digraph
from layers import CTRNNLayer

import warnings
		
# CTRNN Network
class CTRNN:
	"""
	The CTRNN Neural Network Class
	The network has an adjustable time interval and
	time constants of each neuron
	
	...
	
	Attributes
	----------
	layer_dimensions: array_like
		The layer_dimensions specifies the parameters of the layers of the Neural Network.
		The layout followed is:
		[[number_of_nodes_in_first_layer(input_layer), list_of_time_constants, activation_function], [number_of_nodes_in_second_layer, list_of_time_constants, activation_function], ..., [number_of_nodes_in_output]]
		
	Methods
	-------
	forward_propagate(input_vector)
		Generates the output of the Neural Network given the input_vector
		
	save_parameters_to_file(file_name)
		Saves the parameters of the Neural Network to an external file named as file_name
		
	load_parameters_from_file(file_name)
		Loads the parameters of the Neural Network from an external file named as file_name
		
	return_parameters_as_vectors()
		Returns the parameters of the vector organized in the form of a vector
		
	load_parameters_from_vector(weight_vector)
		Load the parameters of the Neural Network from a user input in the form of an array/vector
		
	generate_visual(filename, view=False)
		Generate the visual representation of the Neural Network
	"""
	def __init__(self, layer_dimensions, time_interval):
		# Make class declarations
		self.input_dim = (layer_dimensions[0][0], )	

		# Initialize a layer vector, a list of Layer objects
		self.layer_vector = []
		
		# Append the Layer classes
		for i in range(len(layer_dimensions) - 1):
			self.layer_vector.append(CTRNNLayer(layer_dimensions[i][0], layer_dimensions[i+1][0], time_interval, layer_dimensions[i][1], layer_dimensions[i][2], "Layer " + str(i)))
			
		# Number of layers
		self.number_of_layers = len(self.layer_vector)
		# Construct a visual component
		self.visual = Digraph(comment="Continous Time Recurrent Neural Network", graph_attr={'rankdir': "LR", 'splines': "line"}, node_attr={'fixedsize': "true", 'label': ""})
		
	# Function to get output from input_vector
	def forward_propagate(self, input_vector):
		"""
		Generate the output of the Neural Network when input_vector
		is passed to the Neural Network
		
		Parameters
		----------
		input_vector: array_like
			The input_vector to be passed to the Neural Network
			
		Returns
		-------
		intermediate_output: array_like
			The output vector that is generated by the Neural Network
			
		Raises
		------
		ValueException
			The input_vector should be of the dimensions of the input of the network
		"""
		# Convert the input_vector to numpy array
		intermediate_output = np.array(input_vector)
		
		# Check dimensions
		if(intermediate_output.shape != self.input_dim):
			raise ValueError("The input dimensions do not match!")
		
		# Forward propagate for each layer
		for layer in self.layer_vector:
			intermediate_output = layer.euler_step(intermediate_output)
			
		return intermediate_output

	# Function to save the layer parameters
	def save_parameters_to_file(self, file_name):
		"""
		Save the parameters of the Neural Network
		
		Using pickle, the list of layers is stored
		"""
		# Use pickle to save the layer_vector
		with open(file_name, 'wb') as f:
			pickle.dump(self.layer_vector, f)
			
	# Function to load the layer parameters
	def load_parameters_from_file(self, file_name):
		""" Load the parameters of the Neural Network """
		# Use pickle to load the layer_vector
		with open(file_name, 'rb') as f:
			self.layer_vector = pickle.load(f)
			
	# Function to return the parameters in the form of a vector
	def return_parameters_as_vector(self):
		"""
		Return the parameters of the CTRNN Neural Network in the form of
		a an array / vector.
		
		Parameters
		----------
		None
		
		Returns
		-------
		output: array_like
			The vector representation of the parameters of the Neural Network
			
		Raises
		------
		None
		
		Notes
		-----
		The numpy flatten function works in row major order.
		The parameter vector follows the layout as
		[w_11, w_21, w_12, w_22, w_13, w_23, b_1, b_2, b_3, a_1g, a_1b, a_2g, a_2b, a_3g, a_3b, w_11, w_21, w_31, b_1, ...]
		Here, w_ij implies the weight between ith input node and jth output node. b_i is the bias for the ith output node.
		a_ib is the bias activation parameter of ith output node and a_ig is the gain activation parameter of ith output node.
		"""
		# Initialize the output vector
		# Determine an individual layer's weight matrix in row major form, it's bias and then activation function parameters
		# Then concatenate it with the previous output vector
		output = np.array([])
	
		for layer in self.layer_vector:
			# The vector we get from flattening the weight matrix
			# flatten() works in row major order
			weight_vector = layer.get_weight_matrix().flatten()
			
			# The vector we get from flattening the bias vector
			bias_vector = layer.get_bias_vector().flatten()
			
			# The vector of activation parameters
			activation_vector = np.array(layer.get_activation_parameters())
			
			# The output vector is concatenated form of weight_vector, bias_vector and activation_vector
			output = np.concatenate([output, weight_vector, bias_vector, activation_vector])
		
		return output
	
	# Function to load the parameters from vector
	def load_parameters_from_vector(self, parameter_vector):
		"""
		Load the parameters of the CTRNN Neural Network in the form of
		an array / vector
		
		Parameters
		----------
		parameter_vector: array_like
			The parameter vector follows the layout as
			[w_11, w_21, w_12, w_22, w_13, w_23, b_1, b_2, b_3, a_1g, a_1b, a_2g, a_2b, a_3g, a_3b, w_11, w_21, w_31, b_1, ...]
			Here, w_ij implies the weight between ith input node and jth output node. b_i is the bias for the ith output node.
			a_ib is the bias activation parameter of ith output node and a_ig is the gain activation parameter of ith output node. 
			
		Returns
		-------
		None
		
		Raises
		------
		ValueException
			The parameter array is shorter than required
			
		Warning
			The parameter array is greater than required
		"""
		# Convert to numpy array
		parameter_vector = np.array(parameter_vector)
	
		# Interval counter maintains the current layer index
		interval_counter = 0
		
		for layer in self.layer_vector:
			# Get the dimensions of the weight matrix and bias vector
			weight_dim = layer.get_weight_dim()
			bias_dim = layer.get_bias_dim()
			
			# Get the interval at which weight and bias seperate
			weight_interval = weight_dim[0] * weight_dim[1]
			
			# Get the interval at which the bias and next weight vector seperate
			bias_interval = bias_dim[0]
			
			# Seperate the weights and bias and then reshape them
			# Numpy raises a None Type Exception, as it cannot reshape a None object
			# If such an excpetion occurs, raise a value error as our parameter_vector
			# is shorter than required
			try:
				layer.set_weight_matrix(parameter_vector[interval_counter:interval_counter + weight_interval].reshape(weight_dim))
				interval_counter = interval_counter + weight_interval
				layer.set_bias_vector(parameter_vector[interval_counter:interval_counter + bias_interval].reshape(bias_dim[0],))
				interval_counter = interval_counter + bias_interval
				layer.set_activation_parameters(parameter_vector[interval_counter], parameter_vector[interval_counter + 1])
				interval_counter = interval_counter + 2
			except:
				raise ValueError("The parameter_vector consists of elements less than required")
			
		# The interval counter should contain the number of elements in parameter_vector
		# Otherwise the user has specified parameters more than required
		# Just a warning is enough
		if(len(parameter_vector) > interval_counter):
			warnings.warn("The parameter vector consists of elements greater than required")
			
	
	# Function to generate the visual representation
	def generate_visual(self, filename, view=False):
		"""
		Generate the visual representation of the Neural Network and
		store it as a pdf file in the representations directory
		
		Parameters
		----------
		filename
			Specifies the name of the pdf file which is to be saved
			
		view=False
			Whether to view the generated file(True) or not(False)
			
		Returns
		-------
		None
		
		Raises
		------
		None
		
		Note
		----
		Graphviz library is used to generate the representation
		"""
		# We need many subgraphs
		for layer in range(self.number_of_layers):
			subgraph = Digraph(name="cluster_" + str(layer), graph_attr={'color': "white", 'label': "Layer " + str(layer)}, node_attr={'style': "solid", 'color': "black", 'shape': "circle"})
			
			# Get the weight dimensions for generating the nodes
			weight_dim = self.layer_vector[layer].get_weight_dim()
			
			# Get the time constants
			if layer != 0:
				time_constants = self.layer_vector[layer-1].get_time_constant()
			
			for node_number in range(weight_dim[1]):
				if layer == 0:
					subgraph.node("layer_" + str(layer) + str(node_number+1))
				else:
					subgraph.node("layer_" + str(layer) + str(node_number+1), label=str(time_constants[node_number]), fontcolor='green')
				
			# Declare subgraphs
			self.visual.subgraph(subgraph)
			
			
		# The final layer needs to be done manually
		subgraph = Digraph(name="cluster_" + str(self.number_of_layers), graph_attr={'color': "white", 'label': "Layer " + str(self.number_of_layers)}, node_attr={'style': "solid", 'color': "black", 'shape': "circle"})
		
		# Get the weight dimensions
		weight_dim = self.layer_vector[self.number_of_layers - 1].get_weight_dim()
		
		# Get the time constants
		time_constants = self.layer_vector[self.number_of_layers - 1].get_time_constant()
		
		for node_number in range(weight_dim[0]):
			subgraph.node("layer_" + str(self.number_of_layers) + str(node_number+1), label=str(time_constants[node_number]), fontcolor='green')
			
		# Declare the subgraph
		self.visual.subgraph(subgraph)
		
		
		for layer in range(self.number_of_layers):
			# Get the weight dimensions for generating the nodes
			weight_dim = self.layer_vector[layer].get_weight_dim()
		
			# Put the edges in the graph
			for input_node in range(weight_dim[1]):
				for output_node in range(weight_dim[0]):
					self.visual.edge("layer_" + str(layer) + str(input_node+1), 'layer_' + str(layer + 1) + str(output_node+1))
		
		# Render the graph		
		self.visual.render('representations/' + filename + '.gv', view=view)		
	
		
		
		
		
		
